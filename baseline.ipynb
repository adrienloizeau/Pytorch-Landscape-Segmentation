{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/adrienloizeau/baseline?scriptVersionId=114104441\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"---\n\nBelow is a sample workflow to help you get started. The score obtained from it is around the baseline. At the end there are a few suggestions where you could go from here.\n\n---","metadata":{"papermill":{"duration":0.008291,"end_time":"2022-12-05T10:40:02.759785","exception":false,"start_time":"2022-12-05T10:40:02.751494","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\n\nimport numpy as np\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport glob\nfrom tqdm import tqdm\nimport time\nimport os","metadata":{"papermill":{"duration":1.878071,"end_time":"2022-12-05T10:40:04.64321","exception":false,"start_time":"2022-12-05T10:40:02.765139","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-17T23:45:22.221904Z","iopub.execute_input":"2022-12-17T23:45:22.222272Z","iopub.status.idle":"2022-12-17T23:45:22.230694Z","shell.execute_reply.started":"2022-12-17T23:45:22.222238Z","shell.execute_reply":"2022-12-17T23:45:22.229759Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class config:\n    BASE_PATH = \"../input/msc-ai-2022/\"\n    TRAIN_IMG_PATH = \"/kaggle/input/0feff4fbc0c2233153baa7b6c6c00815/train_images\"\n    TRAIN_MASK_PATH = \"/kaggle/input/msc-ai-2022/train_masks/train_masks\"\n    TEST_IMG_PATH = \"/kaggle/input/0feff4fbc0c2233153baa7b6c6c00815/test_images\"","metadata":{"papermill":{"duration":0.013936,"end_time":"2022-12-05T10:40:04.662369","exception":false,"start_time":"2022-12-05T10:40:04.648433","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-17T23:45:22.236651Z","iopub.execute_input":"2022-12-17T23:45:22.236919Z","iopub.status.idle":"2022-12-17T23:45:22.242074Z","shell.execute_reply.started":"2022-12-17T23:45:22.236895Z","shell.execute_reply":"2022-12-17T23:45:22.241022Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# 1. Dataloaders","metadata":{"papermill":{"duration":0.004853,"end_time":"2022-12-05T10:40:04.672069","exception":false,"start_time":"2022-12-05T10:40:04.667216","status":"completed"},"tags":[]}},{"cell_type":"code","source":"HEIGHT = 256\nWIDTH = 256\nBATCH_SIZE = 8","metadata":{"papermill":{"duration":0.012144,"end_time":"2022-12-05T10:40:04.68911","exception":false,"start_time":"2022-12-05T10:40:04.676966","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-17T23:45:22.246452Z","iopub.execute_input":"2022-12-17T23:45:22.246873Z","iopub.status.idle":"2022-12-17T23:45:22.253275Z","shell.execute_reply.started":"2022-12-17T23:45:22.246846Z","shell.execute_reply":"2022-12-17T23:45:22.252305Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Create a custom Dataset class\nclass tensorDataset(Dataset):\n    def __init__(self, image_paths: list, mask_paths: list, train: bool):\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.train = train\n        \n        self.transform_img = transforms.Compose([\n            transforms.Resize([HEIGHT,WIDTH]),\n            #transforms.RandomCrop([HEIGHT,WIDTH]),\n\n            transforms.ToTensor()\n        ])\n        self.transform_mask = transforms.Compose([\n            transforms.Resize([HEIGHT,WIDTH], interpolation = transforms.InterpolationMode.NEAREST), # Important\n            #transforms.RandomCrop([HEIGHT,WIDTH]),\n#            transforms.RandomHorizontalFlip(p=0.2),\n            transforms.ToTensor()\n        ])\n        \n    def __getitem__(self, index):\n        \n        # Select a specific image's path\n        img_path  = self.image_paths[index]\n        mask_path = self.mask_paths[index]\n        \n        # Load the image\n        img = Image.open(img_path)\n        mask = Image.open(mask_path)\n    \n        # Apply transformations\n        img = self.transform_img(img)\n        mask = self.transform_mask(mask)\n        \n        # Scale the mask from 0-1 range to 0-255 range\n        mask = mask * 255  \n        \n        # Reshape mask from (1, H, W) to (H, W) -> This is because loss function accepts (B, H, W) not (B, 1, H, W)\n        mask = mask.squeeze(0)\n        \n        return img, mask\n\n    def __len__(self):\n        return len(self.image_paths)","metadata":{"papermill":{"duration":0.016424,"end_time":"2022-12-05T10:40:04.710478","exception":false,"start_time":"2022-12-05T10:40:04.694054","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-17T23:45:22.257273Z","iopub.execute_input":"2022-12-17T23:45:22.259067Z","iopub.status.idle":"2022-12-17T23:45:22.268297Z","shell.execute_reply.started":"2022-12-17T23:45:22.259038Z","shell.execute_reply":"2022-12-17T23:45:22.267458Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Very simple train/test split\ntrain_ratio = 0.8\ntrain_set_last_idx = int(len(glob.glob(config.TRAIN_IMG_PATH + \"/*\")) * train_ratio)\n\ntrain_img_paths  = sorted(glob.glob(config.TRAIN_IMG_PATH + \"/*\"))[:train_set_last_idx]\ntrain_mask_paths = sorted(glob.glob(config.TRAIN_MASK_PATH + \"/*\"))[:train_set_last_idx]\nval_img_paths    = sorted(glob.glob(config.TRAIN_IMG_PATH + \"/*\"))[train_set_last_idx:]\nval_mask_paths   = sorted(glob.glob(config.TRAIN_MASK_PATH + \"/*\"))[train_set_last_idx:]\n\n# Create datasets\ntrain_dataset = tensorDataset(train_img_paths, train_mask_paths, train=True)\nval_dataset   = tensorDataset(val_img_paths, val_mask_paths, train=False)\n\n# Create dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory = True)\nval_dataloader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory = True)","metadata":{"papermill":{"duration":0.086694,"end_time":"2022-12-05T10:40:04.802433","exception":false,"start_time":"2022-12-05T10:40:04.715739","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-17T23:45:22.272557Z","iopub.execute_input":"2022-12-17T23:45:22.272859Z","iopub.status.idle":"2022-12-17T23:45:22.294659Z","shell.execute_reply.started":"2022-12-17T23:45:22.272834Z","shell.execute_reply":"2022-12-17T23:45:22.293544Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test dataloaders\nstart = time.time()\nfor batch in tqdm(train_dataloader):\n    img_batch, img_mask = batch\n\nfor batch in tqdm(val_dataloader):\n    img_batch, img_mask = batch\n      \nend = time.time()\nprint(f'Seconds needed to load one train + val epoch: {end - start :.3f}')","metadata":{"execution":{"iopub.status.busy":"2022-12-17T23:45:22.35101Z","iopub.execute_input":"2022-12-17T23:45:22.352983Z","iopub.status.idle":"2022-12-17T23:45:33.532407Z","shell.execute_reply.started":"2022-12-17T23:45:22.352955Z","shell.execute_reply":"2022-12-17T23:45:33.531309Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"100%|██████████| 26/26 [00:08<00:00,  2.96it/s]\n100%|██████████| 7/7 [00:02<00:00,  2.94it/s]","output_type":"stream"},{"name":"stdout","text":"Seconds needed to load one train + val epoch: 11.171\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The dataloaders are extremely slow! It takes around a minute to load one epoch, which can be a huge overhead if you want to train for e.g. 100 epochs. The reason is that the images are quite large (some around 3000x4000) and loading such images takes a lot of time. Ideally, for this dataset you should have dataloaders that load one epoch in a few seconds at max. To achieve this we can do a few things:\n\n1. We could pre-process the images. Reszing the images beforehand to the desired height and width, will significantly decrease the loading time.\n2. We could increase the number of workers. However, remember that the more batches you store at once, the less memory you have for other operations.","metadata":{"papermill":{"duration":0.007769,"end_time":"2022-12-05T10:41:58.115422","exception":false,"start_time":"2022-12-05T10:41:58.107653","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 2. Model definition","metadata":{"papermill":{"duration":0.007495,"end_time":"2022-12-05T10:41:58.130605","exception":false,"start_time":"2022-12-05T10:41:58.12311","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n    def __init__(self, in_channels: int, forward_expansion: int,  out_channels: int, expand: bool):\n        super(BasicBlock, self).__init__()\n        \"\"\"\n        A very simple convlution block. Reduces or expands the size of the image by a factor of 2.\n        When using batchnorm, you can set bias=False to preceding convolution.\n        \"\"\"\n        self.conv1 = nn.Conv2d(in_channels, forward_expansion, 3, stride=1, padding='same', bias=False)\n        self.bn1 = nn.BatchNorm2d(forward_expansion)\n        self.conv2 = nn.Conv2d(forward_expansion, out_channels, 3, stride=1, padding='same', bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        if expand:\n            self.scaling = nn.ConvTranspose2d(out_channels, out_channels, kernel_size=2, stride=2)\n        else:\n            self.scaling = nn.MaxPool2d(kernel_size = 2, stride = 2)\n        \n    def forward(self, x):\n        x = self.bn1(self.conv1(x))\n        x = F.relu(x)\n        x = self.bn2(self.conv2(x))\n        x = F.relu(x)\n        x = self.scaling(x)\n        return x\n\n    \nclass SampleModel(nn.Module):\n    def __init__(self, in_channels=3, out_channels=25):\n        super(SampleModel, self).__init__()\n        \n        # Create downsizing pass\n        self.down = nn.ModuleList()\n        self.down.append(BasicBlock(3, 256, 256, expand=False))\n        self.down.append(BasicBlock(256, 512, 512, expand=False))\n        self.down.append(BasicBlock(512, 512, 1024, expand=False))\n        \n        # Create bottleneck\n        self.bottleneck = nn.Conv2d(1024, 1024, 3, stride=1, padding='same')\n        \n        # Create upsizing pass\n        self.up = nn.ModuleList()\n        self.up.append(BasicBlock(1024, 512, 512, expand=True))\n        self.up.append(BasicBlock(512, 512, 256, expand=True))\n        self.up.append(BasicBlock(256, 256, out_channels, expand=True))\n        \n    def forward(self, x):\n        for block in self.down:\n            x = block(x)\n        x = self.bottleneck(x)\n        for block in self.up:\n            x = block(x)\n        return x","metadata":{"papermill":{"duration":0.026121,"end_time":"2022-12-05T10:41:58.164357","exception":false,"start_time":"2022-12-05T10:41:58.138236","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-17T23:45:33.535232Z","iopub.execute_input":"2022-12-17T23:45:33.535667Z","iopub.status.idle":"2022-12-17T23:45:33.549678Z","shell.execute_reply.started":"2022-12-17T23:45:33.535622Z","shell.execute_reply":"2022-12-17T23:45:33.54811Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# 3. Training loop","metadata":{"papermill":{"duration":0.007143,"end_time":"2022-12-05T10:41:58.178603","exception":false,"start_time":"2022-12-05T10:41:58.17146","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def train_acc(model, epochs, optimizer, criterion):\n    \n    for epoch in range(epochs):\n        \n        train_losses = []\n        train_accuracy = []\n        val_losses = []\n        val_accuracy = []\n      \n        ###### Train model ######\n        model.train()\n        for i, batch in enumerate(train_dataloader):\n            # Extract images and masks\n            img_batch, mask_batch = batch  # img [B,3,H,W], mask[B,H,W]\n            img_batch, mask_batch = img_batch.to(device), mask_batch.long().to(device)\n            \n            # Optimize network\n            optimizer.zero_grad()\n            output = model(img_batch) # output: [B, 25, H, W]\n            loss = criterion(output, mask_batch)\n            loss.backward()\n            optimizer.step()\n          \n            # Save batch results\n            train_losses.append(loss.item())\n            preds = torch.argmax(output, dim=1)\n            acc = torch.sum(preds == mask_batch).item() / (mask_batch.shape[0] * mask_batch.shape[1] * mask_batch.shape[2])\n            # we divide by (batch_size * height * width) to get average accuracy per pixel\n            train_accuracy.append(acc)\n\n        ###### Validate model ######\n        model.eval()      \n        for i, batch in enumerate(val_dataloader):\n            #Extract data, labels\n            img_batch, mask_batch = batch  # img [B,3,H,W], mask[B,H,W]\n            img_batch, mask_batch = img_batch.to(device), mask_batch.long().to(device)\n\n            #Validate model\n            output = model(img_batch)\n            loss = criterion(output, mask_batch)\n\n            # Save batch results\n            val_losses.append(loss.item())\n            preds = torch.argmax(output, dim=1)\n            acc = torch.sum(preds == mask_batch).item() / (mask_batch.shape[0] * mask_batch.shape[1] * mask_batch.shape[2])\n            val_accuracy.append(acc)\n      \n        ##### Print epoch results ######\n        print(f'TRAIN       Epoch: {epoch} | Epoch metrics | loss: {np.mean(train_losses):.4f}, accuracy: {np.mean(train_accuracy):.3f}')        \n        print(f'VALIDATION  Epoch: {epoch} | Epoch metrics | loss: {np.mean(val_losses):.4f}, accuracy: {np.mean(val_accuracy):.3f}')\n        print('-' * 70)","metadata":{"papermill":{"duration":0.023322,"end_time":"2022-12-05T10:41:58.209173","exception":false,"start_time":"2022-12-05T10:41:58.185851","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-17T23:45:33.551358Z","iopub.execute_input":"2022-12-17T23:45:33.552095Z","iopub.status.idle":"2022-12-17T23:45:33.566628Z","shell.execute_reply.started":"2022-12-17T23:45:33.552058Z","shell.execute_reply":"2022-12-17T23:45:33.565646Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"With the dice metric","metadata":{}},{"cell_type":"code","source":"# With the dice metric\nfrom torchmetrics.functional import dice as Dice\n\ndef train(model, epochs, optimizer, criterion):\n    \n    for epoch in range(epochs):\n        \n        train_losses = []\n        train_dice = []\n        train_accuracy = [] \n        val_losses = []\n        val_dice = []\n        val_accuracy = []\n      \n        ###### Train model ######\n        model.train()\n        for i, batch in enumerate(train_dataloader):\n            # Extract images and masks\n            img_batch, mask_batch = batch  # img [B,3,H,W], mask[B,H,W]\n            img_batch, mask_batch = img_batch.to(device), mask_batch.long().to(device)\n            \n            # Optimize network\n            optimizer.zero_grad()\n            output = model(img_batch) # output: [B, 25, H, W]\n            loss = criterion(output, mask_batch)\n            loss.backward()\n            optimizer.step()\n          \n            # Save batch results\n            train_losses.append(loss.item())\n            preds = torch.argmax(output, dim=1)\n            dice = Dice(preds, mask_batch)\n            train_dice.append(dice.cpu())\n            acc = torch.sum(preds == mask_batch).item() / (mask_batch.shape[0] * mask_batch.shape[1] * mask_batch.shape[2])\n            # we divide by (batch_size * height * width) to get average accuracy per pixel\n            train_accuracy.append(acc)\n\n        ###### Validate model ######\n        model.eval()      \n        for i, batch in enumerate(val_dataloader):\n            #Extract data, labels\n            img_batch, mask_batch = batch  # img [B,3,H,W], mask[B,H,W]\n            img_batch, mask_batch = img_batch.to(device), mask_batch.long().to(device)\n\n            #Validate model\n            output = model(img_batch)\n            loss = criterion(output, mask_batch)\n\n            # Save batch results\n            val_losses.append(loss.item())\n            preds = torch.argmax(output, dim=1)\n            acc = torch.sum(preds == mask_batch).item() / (mask_batch.shape[0] * mask_batch.shape[1] * mask_batch.shape[2])\n            val_accuracy.append(acc)\n            dice = Dice(preds, mask_batch)\n            val_dice.append(dice.cpu())\n        ##### Print epoch results ######\n        print(f'TRAIN       Epoch: {epoch} | Epoch metrics | loss: {np.mean(train_losses):.4f}, dice: {np.mean(train_dice):.3f}, accuracy: {np.mean(train_accuracy):.3f}')        \n        print(f'VALIDATION  Epoch: {epoch} | Epoch metrics | loss: {np.mean(val_losses):.4f}, dice: {np.mean(val_dice):.3f}, accuracy: {np.mean(val_accuracy):.3f}')\n        print('-' * 70)","metadata":{"execution":{"iopub.status.busy":"2022-12-17T23:45:33.569521Z","iopub.execute_input":"2022-12-17T23:45:33.569883Z","iopub.status.idle":"2022-12-17T23:45:33.583947Z","shell.execute_reply.started":"2022-12-17T23:45:33.569848Z","shell.execute_reply":"2022-12-17T23:45:33.58287Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nepochs = 15\nlr = 0.0001\nmodel = SampleModel(in_channels=3, out_channels=25).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss().to(device)","metadata":{"papermill":{"duration":3.354394,"end_time":"2022-12-05T10:42:01.570949","exception":false,"start_time":"2022-12-05T10:41:58.216555","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-17T23:48:05.487849Z","iopub.execute_input":"2022-12-17T23:48:05.488218Z","iopub.status.idle":"2022-12-17T23:48:05.860169Z","shell.execute_reply.started":"2022-12-17T23:48:05.488185Z","shell.execute_reply":"2022-12-17T23:48:05.859184Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train(model, epochs, optimizer, criterion)","metadata":{"papermill":{"duration":1538.138066,"end_time":"2022-12-05T11:07:39.716231","exception":false,"start_time":"2022-12-05T10:42:01.578165","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-17T23:48:05.861993Z","iopub.execute_input":"2022-12-17T23:48:05.86235Z","iopub.status.idle":"2022-12-17T23:53:10.529877Z","shell.execute_reply.started":"2022-12-17T23:48:05.862311Z","shell.execute_reply":"2022-12-17T23:53:10.52876Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"TRAIN       Epoch: 0 | Epoch metrics | loss: 3.0934, dice: 0.151, accuracy: 0.151\nVALIDATION  Epoch: 0 | Epoch metrics | loss: 3.1627, dice: 0.098, accuracy: 0.098\n----------------------------------------------------------------------\nTRAIN       Epoch: 1 | Epoch metrics | loss: 2.9924, dice: 0.253, accuracy: 0.253\nVALIDATION  Epoch: 1 | Epoch metrics | loss: 3.0240, dice: 0.250, accuracy: 0.250\n----------------------------------------------------------------------\nTRAIN       Epoch: 2 | Epoch metrics | loss: 2.9399, dice: 0.306, accuracy: 0.306\nVALIDATION  Epoch: 2 | Epoch metrics | loss: 2.9645, dice: 0.290, accuracy: 0.290\n----------------------------------------------------------------------\nTRAIN       Epoch: 3 | Epoch metrics | loss: 2.8911, dice: 0.345, accuracy: 0.345\nVALIDATION  Epoch: 3 | Epoch metrics | loss: 2.8794, dice: 0.356, accuracy: 0.356\n----------------------------------------------------------------------\nTRAIN       Epoch: 4 | Epoch metrics | loss: 2.8332, dice: 0.403, accuracy: 0.403\nVALIDATION  Epoch: 4 | Epoch metrics | loss: 2.9001, dice: 0.375, accuracy: 0.375\n----------------------------------------------------------------------\nTRAIN       Epoch: 5 | Epoch metrics | loss: 2.7852, dice: 0.486, accuracy: 0.486\nVALIDATION  Epoch: 5 | Epoch metrics | loss: 2.7783, dice: 0.495, accuracy: 0.495\n----------------------------------------------------------------------\nTRAIN       Epoch: 6 | Epoch metrics | loss: 2.7361, dice: 0.534, accuracy: 0.534\nVALIDATION  Epoch: 6 | Epoch metrics | loss: 2.7596, dice: 0.478, accuracy: 0.478\n----------------------------------------------------------------------\nTRAIN       Epoch: 7 | Epoch metrics | loss: 2.6999, dice: 0.554, accuracy: 0.554\nVALIDATION  Epoch: 7 | Epoch metrics | loss: 2.6757, dice: 0.523, accuracy: 0.523\n----------------------------------------------------------------------\nTRAIN       Epoch: 8 | Epoch metrics | loss: 2.6455, dice: 0.582, accuracy: 0.582\nVALIDATION  Epoch: 8 | Epoch metrics | loss: 2.7246, dice: 0.519, accuracy: 0.519\n----------------------------------------------------------------------\nTRAIN       Epoch: 9 | Epoch metrics | loss: 2.6059, dice: 0.586, accuracy: 0.586\nVALIDATION  Epoch: 9 | Epoch metrics | loss: 2.6149, dice: 0.488, accuracy: 0.488\n----------------------------------------------------------------------\nTRAIN       Epoch: 10 | Epoch metrics | loss: 2.5554, dice: 0.596, accuracy: 0.596\nVALIDATION  Epoch: 10 | Epoch metrics | loss: 2.6561, dice: 0.528, accuracy: 0.528\n----------------------------------------------------------------------\nTRAIN       Epoch: 11 | Epoch metrics | loss: 2.5018, dice: 0.611, accuracy: 0.611\nVALIDATION  Epoch: 11 | Epoch metrics | loss: 2.5473, dice: 0.531, accuracy: 0.531\n----------------------------------------------------------------------\nTRAIN       Epoch: 12 | Epoch metrics | loss: 2.4545, dice: 0.617, accuracy: 0.617\nVALIDATION  Epoch: 12 | Epoch metrics | loss: 2.5508, dice: 0.565, accuracy: 0.565\n----------------------------------------------------------------------\nTRAIN       Epoch: 13 | Epoch metrics | loss: 2.4152, dice: 0.619, accuracy: 0.619\nVALIDATION  Epoch: 13 | Epoch metrics | loss: 2.4984, dice: 0.529, accuracy: 0.529\n----------------------------------------------------------------------\nTRAIN       Epoch: 14 | Epoch metrics | loss: 2.3696, dice: 0.621, accuracy: 0.621\nVALIDATION  Epoch: 14 | Epoch metrics | loss: 2.4909, dice: 0.480, accuracy: 0.480\n----------------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 4. Error Analysis","metadata":{"papermill":{"duration":0.007443,"end_time":"2022-12-05T11:07:39.73193","exception":false,"start_time":"2022-12-05T11:07:39.724487","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Transfer model from gpu to cpu\nmodel = model.cpu()","metadata":{"papermill":{"duration":0.168181,"end_time":"2022-12-05T11:07:39.907682","exception":false,"start_time":"2022-12-05T11:07:39.739501","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-17T23:45:33.989043Z","iopub.status.idle":"2022-12-17T23:45:33.989529Z","shell.execute_reply.started":"2022-12-17T23:45:33.989278Z","shell.execute_reply":"2022-12-17T23:45:33.989302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualise_model_performance(model: nn.Module, idx: int):\n    \n    # Load image\n    img_paths = sorted(glob.glob(config.TRAIN_IMG_PATH + \"/*\"))\n    mask_paths = sorted(glob.glob(config.TRAIN_MASK_PATH + \"/*\"))\n    img_path = img_paths[idx]\n    mask_path = mask_paths[idx]\n    img = Image.open(img_path)\n    true_mask = Image.open(mask_path)\n    \n    # Resize to the same shape as used in training\n    img = train_dataset.transform_img(img)\n    true_mask = train_dataset.transform_mask(true_mask)\n    \n    # Model accepts batches of images, not single images so we need to reshape tensors\n    # From [C, H, W] to [B, C, H, W]\n    img = img.unsqueeze(0)\n    \n    # Get predictions\n    output = model(img)\n    preds = torch.argmax(output, dim=1)\n    \n    # Tranform to numpy and reshape for plotting\n    img = img.squeeze().permute(1,2,0).detach().numpy()\n    preds = preds.permute(1,2,0).detach().numpy()\n    true_mask = true_mask.permute(1,2,0).detach().numpy()\n    \n    # Plot\n    f, axarr = plt.subplots(1,3, figsize=(15,5))\n    axarr[0].imshow(img)\n    axarr[1].imshow(preds, cmap='tab20')\n    axarr[2].imshow(true_mask, cmap='tab20')\n    axarr[0].set_title('Image')\n    axarr[1].set_title('Model prediction')\n    axarr[2].set_title('True mask')\n    for ax in axarr:\n        ax.set_xticks([])\n        ax.set_yticks([])\n    plt.show()","metadata":{"papermill":{"duration":0.021282,"end_time":"2022-12-05T11:07:39.936821","exception":false,"start_time":"2022-12-05T11:07:39.915539","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-17T23:45:33.992256Z","iopub.status.idle":"2022-12-17T23:45:33.993313Z","shell.execute_reply.started":"2022-12-17T23:45:33.993043Z","shell.execute_reply":"2022-12-17T23:45:33.993069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualise_model_performance(model, 10)\nvisualise_model_performance(model, 100)\nvisualise_model_performance(model, 200)","metadata":{"papermill":{"duration":12.935869,"end_time":"2022-12-05T11:07:52.880306","exception":false,"start_time":"2022-12-05T11:07:39.944437","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-17T23:45:33.994456Z","iopub.status.idle":"2022-12-17T23:45:33.995325Z","shell.execute_reply.started":"2022-12-17T23:45:33.995058Z","shell.execute_reply":"2022-12-17T23:45:33.995084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model is starting the get the hang of things but clearly there is still a lot of room for improvement.","metadata":{"papermill":{"duration":0.015596,"end_time":"2022-12-05T11:07:52.913135","exception":false,"start_time":"2022-12-05T11:07:52.897539","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 5. Submission","metadata":{"papermill":{"duration":0.014802,"end_time":"2022-12-05T11:07:52.942984","exception":false,"start_time":"2022-12-05T11:07:52.928182","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!mkdir /kaggle/working/test_predictions","metadata":{"papermill":{"duration":1.004453,"end_time":"2022-12-05T11:07:53.962329","exception":false,"start_time":"2022-12-05T11:07:52.957876","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-17T23:45:33.996734Z","iopub.status.idle":"2022-12-17T23:45:33.99758Z","shell.execute_reply.started":"2022-12-17T23:45:33.997317Z","shell.execute_reply":"2022-12-17T23:45:33.997341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load and transform test images\ntest_img_link_list = sorted(glob.glob(config.TEST_IMG_PATH + '/*'))\ntorch_list = [val_dataset.transform_img(Image.open(link)) for link in test_img_link_list]","metadata":{"papermill":{"duration":39.123609,"end_time":"2022-12-05T11:08:33.10135","exception":false,"start_time":"2022-12-05T11:07:53.977741","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-17T23:45:33.998974Z","iopub.status.idle":"2022-12-17T23:45:33.999844Z","shell.execute_reply.started":"2022-12-17T23:45:33.999562Z","shell.execute_reply":"2022-12-17T23:45:33.999587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\n# calculate output for each image in test set and save the prediction in new folder\nfor i in tqdm(range(len(torch_list))):\n    img_id = test_img_link_list[i].split('/')[-1].split('.')[0]\n    img = torch_list[i].unsqueeze(0)\n    output = model(img)\n    output = torch.argmax(output, dim=1).squeeze(0)\n    output = np.uint8(output)\n    output = Image.fromarray(output)\n    output.save(f\"/kaggle/working/test_predictions/{img_id}.png\")","metadata":{"papermill":{"duration":368.081108,"end_time":"2022-12-05T11:14:41.198097","exception":false,"start_time":"2022-12-05T11:08:33.116989","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-17T23:45:34.001242Z","iopub.status.idle":"2022-12-17T23:45:34.002249Z","shell.execute_reply.started":"2022-12-17T23:45:34.001974Z","shell.execute_reply":"2022-12-17T23:45:34.002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef rle_decode(mask_rle, shape):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return\n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)\n\n\ndef create_rles():\n    \"\"\"Used for Kaggle submission: predicts and encode all test images\"\"\"\n    dir = '/kaggle/working/test_predictions/'  # CHANGE DIRECTORY IF NEEDED\n    N = len(list(os.listdir(dir)))\n    with open('submission_file.csv', 'w') as f:\n        f.write('ImageClassId,rle_mask\\n')\n        for index, i in enumerate(os.listdir(dir)):\n            # print('{}/{}'.format(index, N))\n                \n            print(i)\n            if i[0] != '.':\n                mask = Image.open(dir + i)\n                mask = mask.resize((1024, 1024), resample=Image.NEAREST)\n                mask = np.array(mask)\n\n                for x in range(1, 25):\n                    enc = rle_encode(mask == x)\n                    f.write(f\"{i.split('_')[0]}_{x},{enc}\\n\")\n\ncreate_rles()","metadata":{"papermill":{"duration":10.942165,"end_time":"2022-12-05T11:14:52.160983","exception":false,"start_time":"2022-12-05T11:14:41.218818","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-12-17T23:45:34.003521Z","iopub.status.idle":"2022-12-17T23:45:34.004376Z","shell.execute_reply.started":"2022-12-17T23:45:34.00413Z","shell.execute_reply":"2022-12-17T23:45:34.004156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How to improve\n\nThe above solution is a very simple pipeline and gives a leaderboard score similar to the basline. From here, you can improve in various ways. This is a good practice in general: start with a simple baseline solution and try to improve from there. Below we outline a couple ideas you can try out but keep in mind that this is not an exhaustive list! It's often the unique ideas that give you an edge in Kaggle challenges.\n\n### 1. Improving the speed of iteration\nOne of the best practices both in industry and on Kaggle is to create a pipeline in which you can iterate ideas quickly. It should take you no more than 20min (for this dataset) to train and evaluate a model. It doesn't mean that your final soultion should be trained in 20mins but it means that if you want to change some model parameters/data variants/training properties, you should be able to assess if your new idea is worth more of your efforts in about that much time. How you could speed up the above timeline:\n- Include preprocessing step. Instead of loading large (4000x3000) images in the dataloader, resize them beforehand\n- Increase number of workers in the dataloader\n- Set up a better evaluation metric than accuracy. Ideally, it should be in line with the evaluation metric of the competition. This will allow your validation set to give a good approximation of your performance on the leaderboard\n- Use GPUs for training. This can drive down your training time from hours to just minutes. Easy two options are: Google Colab (10 euro a month), or Kaggle (free but limit of 33 GPU hours per week - should be enough honestly)\n\n### 2. Improving performance\nOnce you are able to iterate quickly here are a few ideas on how you could improve performance:\n- Perform Exploratory Data Analysis (EDA). Think of the key problems with the dataset and how you could tackle them.\n- Include transformations for you training set. You can use [native PyTorch transformations](https://pytorch.org/vision/stable/transforms.html) or I can recommend the [Albumentations library](https://albumentations.ai/) specifically designed for image transformations. Nice tutorial [here](https://www.youtube.com/watch?v=rAdLwKJBvPM)\n- Try different architectures - Unet is a good start\n- Improve your understanding of the training process. You could plot loss/accuracy/other metrics over time to get an idea of how many epochs you need to reach convergence. You could even use an experiment tracking software such as [Weights & Biases](https://wandb.ai/site)\n- Experiment with different learning rates. You could even use a [learning rate scheduler](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)\n- Experiment with different loss functions\n- Experiment with different image sizes\n- Use transfer learning. This means you would need to obtain a semantic segmentation model from somewhere (no links here :) ) and replace the last layer to output 25 channels (because we have 25 classes in our dataset). Keep in mind that models pretrained on seemingly unrelated datasets like Imagenet can still work here! This is because the lower-level layers learn simple features such as lines, shapes, and colours, which can be useful for our problem as well.\n- Think of a better train/validation split\n\n<br>\n\nThis is already a lot of things to implement. Given that you have limited time, try to implement a few and maybe some of your own ideas and your leaderboard score surely will improve. Good luck!\n","metadata":{"papermill":{"duration":0.023042,"end_time":"2022-12-05T11:14:52.207753","exception":false,"start_time":"2022-12-05T11:14:52.184711","status":"completed"},"tags":[]}}]}